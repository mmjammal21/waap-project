import pandas as pd
import numpy as np
import glob
import os
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø± ---
DATA_PATH = '/home/malik/graduation_project/data/waap_dataset_2026/'
SAVE_PATH = '/home/malik/graduation_project/data'

print("ğŸš€ Starting Optimized AI Training (RAM Safe Mode)...")

# --- 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Smart Sampling) ---
all_files = glob.glob(os.path.join(DATA_PATH, "*.csv"))

if not all_files:
    print(f"âŒ Error: No CSV files found in {DATA_PATH}")
    exit()

# Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ 3 Ù…Ù„ÙØ§Øª ÙÙ‚Ø· Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø­Ù…Ù„
selected_files = all_files[:3]
print(f"ğŸ“‚ Reading data from {len(selected_files)} files...")

df_list = []
for file in selected_files:
    try:
        # Ù†Ù‚Ø±Ø£ ÙÙ‚Ø· 150,000 Ø³Ø·Ø± Ù…Ù† ÙƒÙ„ Ù…Ù„Ù Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù…ØªÙ„Ø§Ø¡ Ø§Ù„Ø±Ø§Ù…
        df = pd.read_csv(file, nrows=150000) 
        df_list.append(df)
    except Exception as e:
        print(f"âš ï¸ Skipped {os.path.basename(file)}: {e}")

if not df_list:
    print("âŒ Failed to load data.")
    exit()

full_df = pd.concat(df_list, ignore_index=True)

# --- 3. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
full_df.columns = full_df.columns.str.strip().str.lower()

# Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
required_features = ['flow_duration', 'header_length', 'protocol_type', 'duration', 'rate']
existing_cols = full_df.columns.tolist()

feature_map = {
    'flow_duration': ['flow duration', 'flow_duration'],
    'header_length': ['header length', 'header_length', 'tot len'],
    'protocol_type': ['protocol', 'protocol type'],
    'duration': ['duration'],
    'rate': ['rate', 'srate']
}

final_features = []
for req in required_features:
    found = False
    for candidate in feature_map.get(req, []):
        if candidate in existing_cols:
            final_features.append(candidate)
            found = True
            break
    if not found:
        full_df[req] = 0
        final_features.append(req)

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù (Label)
label_col = 'label' if 'label' in full_df.columns else 'class'
if not label_col:
    print("âŒ Error: Label column not found.")
    exit()

# --- Ø®Ø·ÙˆØ© Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§Ù‹: ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¶Ø®Ù…Ø© ---
# Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ Ø£ÙƒØ¨Ø± Ù…Ù† 300 Ø£Ù„ÙØŒ Ù†Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¨Ø­Ø¬Ù… 300 Ø£Ù„Ù ÙÙ‚Ø·
# Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ù… Ù„Ù† ØªÙ…ØªÙ„Ø¦
MAX_RECORDS = 300000
if len(full_df) > MAX_RECORDS:
    print(f"âš ï¸ Data is too large ({len(full_df)} records). Sampling {MAX_RECORDS} random records to save RAM...")
    full_df = full_df.sample(n=MAX_RECORDS, random_state=42)

X = full_df[final_features]
y = full_df[label_col]

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

# --- 4. Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ---
print(f"ğŸ“Š Training on {len(X)} records...")

le = LabelEncoder()
y_encoded = le.fit_transform(y.astype(str))

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

print("ğŸ§  Training Random Forest (Optimized)...")
# Ù‚Ù„Ù„Ù†Ø§ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø¬Ø§Ø± (n_estimators) ÙˆØ­Ø¯Ø¯Ù†Ø§ Ø§Ù„Ø¹Ù…Ù‚ (max_depth) Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
model = RandomForestClassifier(n_estimators=30, max_depth=15, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

# --- 5. Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ù„Ø­ÙØ¸ ---
print("\nğŸ” Evaluating...")
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("="*40)
print(f"âœ… MODEL ACCURACY: {accuracy * 100:.2f}%")
print("="*40)

print("\nğŸ’¾ Saving Model...")
joblib.dump(model, os.path.join(SAVE_PATH, 'waap_model.pkl'))
joblib.dump(le, os.path.join(SAVE_PATH, 'label_encoder.pkl'))
print("ğŸ‰ Done! Model is ready and saved.")
