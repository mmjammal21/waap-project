import pandas as pd
import numpy as np
import glob
import os
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø± (Settings) ---
DATA_PATH = '/home/malik/graduation_project/data/waap_dataset_2026/'
SAVE_PATH = '/home/malik/graduation_project/data'

print("ğŸš€ Starting FINAL CONSOLIDATED AI Training...")
print("ğŸ¯ Goal: High Accuracy + RAM Safety")

# --- 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø°ÙƒØ§Ø¡ (Smart Loading) ---
# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª CSV
all_files = glob.glob(os.path.join(DATA_PATH, "*.csv"))
if not all_files:
    print(f"âŒ Error: No CSV files found in {DATA_PATH}")
    exit()

# Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ 3 Ù…Ù„ÙØ§Øª ÙÙ‚Ø· Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø­Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ù… Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ØªÙ†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
selected_files = all_files[:3]
print(f"ğŸ“‚ Reading data from {len(selected_files)} files...")

df_list = []
for file in selected_files:
    try:
        # Ù‚Ø±Ø§Ø¡Ø© 150 Ø£Ù„Ù Ø³Ø·Ø± Ù…Ù† ÙƒÙ„ Ù…Ù„Ù (ØªÙˆØ§Ø²Ù† Ù…Ù…ØªØ§Ø² Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø³Ø±Ø¹Ø©)
        df = pd.read_csv(file, nrows=150000)
        df_list.append(df)
        print(f"  - Loaded: {os.path.basename(file)}")
    except Exception as e:
        print(f"  âš ï¸ Skipped {os.path.basename(file)}: {e}")

if not df_list:
    print("âŒ Failed to load any data.")
    exit()

full_df = pd.concat(df_list, ignore_index=True)

# --- 3. ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (The Master Mapping) ---
# Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙŠØ±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ (Ø§Ù„ÙŠØ³Ø§Ø±) ÙˆØ§Ù„Ø§Ø³Ù… ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¯Ø§ØªØ§ (Ø§Ù„ÙŠÙ…ÙŠÙ†)
# ØªÙ… ØªØ¬Ù…ÙŠØ¹Ù‡ Ù…Ù† ÙƒÙ„ Ù…Ø­Ø§ÙˆÙ„Ø§ØªÙ†Ø§ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø­Ø¯ÙˆØ« Ø®Ø·Ø£
column_mapping = {
    # Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    'flow_duration': 'flow_duration',
    'header_length': 'Header_Length',
    'protocol_type': 'Protocol Type',
    'duration': 'Duration',
    'rate': 'Rate',
    'srate': 'Srate',
    'drate': 'Drate',
    'fin_flag': 'fin_flag_number',
    'syn_flag': 'syn_flag_number',
    'ack_flag': 'ack_flag_number',
    'max_size': 'Max',
    'avg_size': 'AVG',
    'std_dev': 'Std',
    'magnitude': 'Magnitue'
}

print("ğŸ”§ Mapping and cleaning columns...")
final_df = pd.DataFrame()

# Ù…Ø­Ø§ÙˆÙ„Ø© Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
for target_col, source_col in column_mapping.items():
    if source_col in full_df.columns:
        final_df[target_col] = full_df[source_col]
    else:
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ØŒ Ù†Ø¨Ø­Ø« Ø¹Ù†Ù‡ Ø¨Ø­Ø§Ù„Ø© Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø© (Lower Case) ÙƒØ®Ø·Ø© Ø¨Ø¯ÙŠÙ„Ø©
        found = False
        for col in full_df.columns:
            if col.lower() == source_col.lower().replace(' ', '_'):
                final_df[target_col] = full_df[col]
                found = True
                break
        
        if not found:
            # Ø¥Ø°Ø§ ÙŠØ¦Ø³Ù†Ø§ Ù…Ù† Ø¥ÙŠØ¬Ø§Ø¯Ù‡ØŒ Ù†Ù…Ù„Ø£Ù‡ Ø¨ØµÙØ± (Safe Fallback)
            print(f"  âš ï¸ Warning: Column '{source_col}' not found. Filling with 0.")
            final_df[target_col] = 0

# Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ (Label)
found_label = False
for label_candidate in ['label', 'class', 'attack_type', 'Label', 'Class']:
    if label_candidate in full_df.columns:
        final_df['label'] = full_df[label_candidate]
        found_label = True
        break

if not found_label:
    print("âŒ Critical Error: Could not find the 'Label' column!")
    exit()

# --- 4. Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ù… (Sampling) ---
MAX_RECORDS = 300000
if len(final_df) > MAX_RECORDS:
    print(f"âœ‚ï¸ Optimizing dataset: Sampling {MAX_RECORDS} random records...")
    final_df = final_df.sample(n=MAX_RECORDS, random_state=42)

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚ÙŠÙ… (NaN / Infinity)
X = final_df.drop('label', axis=1)
y = final_df['label']
X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

# --- 5. Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (The Engine) ---
print(f"ğŸ“Š Training on {len(X)} records with {len(X.columns)} features...")

le = LabelEncoder()
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (Strings -> Integers)
y_encoded = le.fit_transform(y.astype(str))

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

print("ğŸ§  Building Random Forest Model...")
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø©: 40 Ø´Ø¬Ø±Ø© ÙˆØ¹Ù…Ù‚ 20 (Ù‚ÙˆÙŠ ÙˆØ³Ø±ÙŠØ¹)
model = RandomForestClassifier(n_estimators=30, max_depth=10, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

# --- 6. Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ù„Ø­ÙØ¸ ---
print("\nğŸ” Evaluating Model Accuracy...")
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("="*50)
print(f"âœ… FINAL ACCURACY: {accuracy * 100:.2f}%")
print("="*50)

print("ğŸ’¾ Saving artifacts...")
joblib.dump(model, os.path.join(SAVE_PATH, 'waap_model.pkl'))
joblib.dump(le, os.path.join(SAVE_PATH, 'label_encoder.pkl'))

# Ø­ÙØ¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠ ØªØ¯Ø±Ø¨ Ø¹Ù„ÙŠÙ‡Ø§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù„Ø§Ø­Ù‚Ø§Ù‹
# ØºÙŠØ± compress Ù…Ù† 3 Ø¥Ù„Ù‰ 9
joblib.dump(model, os.path.join(SAVE_PATH, 'waap_model.pkl'), compress=9)

print("ğŸ‰ ALL DONE! The system is ready for the attack simulation.")
